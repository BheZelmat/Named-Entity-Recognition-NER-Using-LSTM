{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39429c4d",
   "metadata": {
    "colab_type": "text",
    "id": "C7oVbe_pLr3C"
   },
   "source": [
    "#  Named Entity Recognition (NER)\n",
    "\n",
    "In this NOtebook, we will learn to: \n",
    "\n",
    "- Design the architecture of a neural network, train it, and test it. \n",
    "- Process features and represents them\n",
    "- Understand word padding\n",
    "- Implement LSTMs\n",
    "- Test with your own sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63264b37",
   "metadata": {
    "colab_type": "text",
    "id": "ftT-5-yynCtl"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "Let's begin by defining what a named entity recognition (NER) is. NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'images/ner.png' width=\"width\" height=\"height\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows: \n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity \n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. You will then evaluate your model and see you get 97% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22352fbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "JEY_jlQQR9SP",
    "outputId": "825f37fd-cf03-483a-a6b1-3d70da6f70f1",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "tf.keras.utils.set_random_seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35b428",
   "metadata": {
    "colab_type": "text",
    "id": "_PpjG5MuLr3F"
   },
   "source": [
    "\n",
    "## 2 - Exploring the Data\n",
    "\n",
    "You will be using a dataset from Kaggle, which  will be preprocessed. The original data consists of four columns: the sentence number, the word, the part of speech of the word (it won't be used in this Notebook), and the tags.  A few tags we might expect to see are: \n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75914c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "-Jur1JnXnCtr",
    "outputId": "88584ab6-0a15-489b-db5b-eb474e129c4f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# display original kaggle data\n",
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "print('ORIGINAL DATA:\\n', data.head())\n",
    "del(data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8f676",
   "metadata": {
    "colab_type": "text",
    "id": "xoH6yBWVfzTb"
   },
   "source": [
    "\n",
    "### 2.1 - Importing the Data\n",
    "\n",
    "In this part, we will import the preprocessed data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323ddf01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path,'r',encoding='UTF8') as file:\n",
    "        data = np.array([line.strip() for line in file.readlines()])\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4dc9e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sentences = load_data('data/large/train/sentences.txt')\n",
    "train_labels = load_data('data/large/train/labels.txt')\n",
    "\n",
    "val_sentences = load_data('data/large/val/sentences.txt')\n",
    "val_labels = load_data('data/large/val/labels.txt')\n",
    "\n",
    "test_sentences = load_data('data/large/test/sentences.txt')\n",
    "test_labels = load_data('data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d817d",
   "metadata": {},
   "source": [
    "\n",
    "## 3 -  Encoding\n",
    "\n",
    "### 3.1 Encoding the sentences\n",
    "\n",
    "In this section, we will use [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) to transform the sentences into integers, so they can be fed into the model we will build later on. \n",
    "\n",
    "\n",
    "The parameter we will need to pass explicitly is `standardize`. This will tell how the parser splits the sentences. By default, `standardize = 'lower_and_strip_punctuation'`, this means the parser will remove all punctuation and make everything lowercase. Note that this may influence the NER task, since an upper case in the middle of a sentence may indicate an entity. Furthermore, the sentences in the dataset are already split into tokens, and all tokens, including punctuation, are separated by a whitespace. The punctuations are also labeled. That said, we will use `standardize = None` so everything will just be split into single tokens and then mapped to a positive integer.\n",
    "\n",
    "Note that `tf.keras.layers.TextVectorization` will also pad the sentences. In this case, it will always pad using the largest sentence in the set you call it with. You will be calling it for the entire training/validation/test set, but padding won't impact at all the model's output, as you will see later on.\n",
    "\n",
    "After instantiating the object, we will need to adapt it to the **sentences training set**, so it will map every token in the training set to an integer. Also, it will by default create two tokens: one for unknown tokens and another for the padding token. Tensorflow maps in the following way:\n",
    "\n",
    "1. padding token: \"\", integer mapped: 0\n",
    "1. unknown token \"[UNK]\", integer mapped: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ec60b8",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_vectorizer(sentences):\n",
    "    tf.keras.utils.set_random_seed(33) \n",
    "    \"\"\"\n",
    "    Create a TextVectorization layer for sentence tokenization and adapt it to the provided sentences.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list of str): Sentences for vocabulary adaptation.\n",
    "\n",
    "    Returns:\n",
    "    sentence_vectorizer (tf.keras.layers.TextVectorization): TextVectorization layer for sentence tokenization.\n",
    "    vocab (list of str): Extracted vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define TextVectorization object with the appropriate standardize parameter\n",
    "    sentence_vectorizer = tf.keras.layers.TextVectorization(standardize = None)\n",
    "    # Adapt the sentence vectorization object to the given sentences\n",
    "    sentence_vectorizer.adapt(sentences)\n",
    "    # Get the vocabulary\n",
    "    vocab = sentence_vectorizer.get_vocabulary()\n",
    "\n",
    "    \n",
    "    return sentence_vectorizer, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ec699",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test vocab size: 4650\n",
      "['', '[UNK]', 'the', '.', ',', 'in', 'of', 'to', 'a', 'and']\n",
      "Sentence: I like learning new NLP models !\n",
      "Sentence vectorized: [ 296  314    1   59    1    1 4649]\n"
     ]
    }
   ],
   "source": [
    "## Let's test our function\n",
    "test_vectorizer, test_vocab = get_sentence_vectorizer(train_sentences[:1000])\n",
    "print(f\"Test vocab size: {len(test_vocab)}\")\n",
    "print(test_vocab[:10])\n",
    "sentence = \"I like learning new NLP models !\"\n",
    "sentence_vectorized = test_vectorizer(sentence)\n",
    "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c869d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49512450",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Encoding the labels\n",
    "\n",
    "In this section we will encode the labels. The process is a bit simpler than encoding the sentences, because there are only a few tags, compared with words in the vocabulary. Note, that there will be one extra tag to represent the padded token that some sentences may have included. Padding will not interfere at all in this task, as we will see further on. Run the next cell to print one example of a tag related to one sentence.\n",
    "\n",
    "Because there is no meaning in having an UNK token for labels and the padding token will be another number different from 0, TextVectorization is not a good choice.\n",
    "\n",
    "We will need also to pad the labels, because the number of labels must match the number of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "727b049f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Labels: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[0]}\")\n",
    "print(f\"Labels: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd4225",
   "metadata": {},
   "source": [
    "We will build the next function to extract all the different tags in a given set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6123a983",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tags(labels):\n",
    "    tag_set = set() # Define an empty set\n",
    "    for el in labels:\n",
    "        for tag in el.split(\" \"):\n",
    "            tag_set.add(tag)\n",
    "    tag_list = list(tag_set) \n",
    "    tag_list.sort()\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb3d4be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = get_tags(train_labels)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b1eea",
   "metadata": {},
   "source": [
    "Now we will need to generate a **tag map**, i.e., a mapping between the tags and **positive** integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88043496",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_tag_map(tags):\n",
    "    tag_map = {}\n",
    "    for i,tag in enumerate(tags):\n",
    "        tag_map[tag] = i \n",
    "    return tag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747233ee",
   "metadata": {},
   "source": [
    "The `tag_map` is a dictionary that maps the tags that you could have to numbers. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity.\n",
    "\n",
    "If you had the sentence \n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "The tags would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "where you would have three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence:\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "Your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcfdscb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
     ]
    }
   ],
   "source": [
    "tag_map = make_tag_map(tags)\n",
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd640d3",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Padding the labels\n",
    "\n",
    "In this section, we will pad the labels. TextVectorization already padded the sentences, so you must ensure that the labels are properly padded as well. This is not a hard task for two main reasons:\n",
    "\n",
    "1. Tensorflow has built-in functions for padding\n",
    "1. Padding will be performed uniformly per dataset (train, validation and test) using the maximum sentence length in each dataset and the size of each sentence is exactly the same as the size of their respective labels.\n",
    "\n",
    "We will pad the vectorized labels with the value -1. We will not use 0 to simplify loss masking and evaluation in further steps. This is because to properly classify one token, a log softmax transformation will be performed and the index with greater value will be the index label. Since index starts at 0, it is better to keep the label 0 as a valid index, even though it is possible to also use 0 as a mask value for labels, but it would require some tweaks in the model architecture or in the loss computation.\n",
    "\n",
    "Tensorflow provides the function [`tf.keras.utils.pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences). The arguments we will need are:\n",
    "\n",
    "- `sequences`: An array with the labels.\n",
    "- `padding`: The position where padding will take place, the standard is `pre`, meaning the sequences will be padded at the beginning. WE need to pass the argument `post`.\n",
    "- `value`: Padding value. The default value is  0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c6804",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Building the label vectorizer\n",
    "\n",
    "Now you're ready to code the label vectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a0a6532",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def label_vectorizer(labels, tag_map):\n",
    "    \"\"\"\n",
    "    Convert list of label strings to padded label IDs using a tag mapping.\n",
    "\n",
    "    Parameters:\n",
    "    labels (list of str): List of label strings.\n",
    "    tag_map (dict): Dictionary mapping tags to IDs.\n",
    "    Returns:\n",
    "    label_ids (numpy.ndarray): Padded array of label IDs.\n",
    "    \"\"\"\n",
    "    label_ids = [] # It can't be a numpy array yet, since each sentence has a different size\n",
    "\n",
    "    # Each element in labels is a string of tags so for each of them:\n",
    "    for element in labels:\n",
    "        # Split it into single tokens. You may use .split function for strings. Be aware to split it by a blank space!\n",
    "        tokens = element.split(\" \")\n",
    "\n",
    "        # Use the dictionaty tag_map passed as an argument to the label_vectorizer function\n",
    "        # to make the correspondence between tags and numbers. \n",
    "        element_ids = []\n",
    "\n",
    "        for token in tokens:\n",
    "            element_ids.append(tag_map[token])\n",
    "\n",
    "        # Append the found ids to corresponding to the current element to label_ids list\n",
    "        label_ids.append(element_ids)\n",
    "        \n",
    "    # Pad the elements\n",
    "    label_ids = tf.keras.utils.pad_sequences(label_ids,\n",
    "                                             padding='post',\n",
    "                                             value=-1 )\n",
    "\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02l23421",
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The party is divided over Britain 's participation in the Iraq conflict and the continued deployment of 8,500 British troops in that country .\n",
      "Labels: O O O O O B-gpe O O O O B-geo O O O O O O O B-gpe O O O O O\n",
      "Vectorized labels: [[16 16 16 16 16  3 16 16 16 16  2 16 16 16 16 16 16 16  3 16 16 16 16 16]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[5]}\")\n",
    "print(f\"Labels: {train_labels[5]}\")\n",
    "print(f\"Vectorized labels: {label_vectorizer([train_labels[5]], tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abf2c8",
   "metadata": {},
   "source": [
    "## 4 Building the Dataset\n",
    "\n",
    "In this section, we will build the dataset for training, validation and testing. We will be using [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class, which provides an optimized way to handle data to feed into a tensorflow model. It may be not as straightforward as a pandas dataset, but it avoids keeping all the data in memory, thus it makes the training faster.\n",
    "\n",
    "We will be using the `tf.data.Dataset.from_tensor_slices` function that converts any iterable into a Tensorflow dataset. You can pass a tuple of `(sentences,labels)` and Tensorflow will understand that each sentence is mapped to its respective label, therefore it is expected that if a tuple of arrays is passed, both arrays have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303db421",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
    "    sentences_ids = sentence_vectorizer(sentences)\n",
    "    labels_ids = label_vectorizer(labels, tag_map = tag_map)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb886f",
   "metadata": {},
   "source": [
    "The next cell will use the function defined above to generate a Tensorflow Dataset for each of the train, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc31ffcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(train_sentences,train_labels, sentence_vectorizer, tag_map)\n",
    "val_dataset = generate_dataset(val_sentences,val_labels,  sentence_vectorizer, tag_map)\n",
    "test_dataset = generate_dataset(test_sentences, test_labels,  sentence_vectorizer, tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a0c9e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "xM9B_Rwxd01i",
    "outputId": "db098ed6-4351-41f7-cfdb-e45dd3798ebf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is 17\n",
      "Num of vocabulary words in the training set: 29847\n",
      "The training size is 33570\n",
      "The validation size is 7194\n",
      "An example of the first sentence is\n",
      "\t [1046    6 1121   18 1832  232  543    7  528    2  158    5   60    9\n",
      "  648    2  922    6  192   87   22   16   54    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "An example of its corresponding label is\n",
      "\t [16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Exploring information about the training data\n",
    "print(f'The number of outputs is {len(tags)}')\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print(f\"Num of vocabulary words in the training set: {g_vocab_size}\")\n",
    "print('The training size is', len(train_dataset))\n",
    "print('The validation size is', len(val_dataset))\n",
    "print('An example of the first sentence is\\n\\t', next(iter(train_dataset))[0].numpy())\n",
    "print('An example of its corresponding label is\\n\\t', next(iter(train_dataset))[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ad50c",
   "metadata": {},
   "source": [
    "\n",
    "### 3.5 - Considerations about RNNs and LSTMs inputs\n",
    "\n",
    "Tensorflow implementation of RNNs (in particular LSTMs) allow you to pass a variable size of input sentences, however this cannot be done **in the same batch**. You must assure that, for each batch, the shapes for our input tensors are the same. \n",
    "\n",
    "A second point here is that, for this purpose, the size of the padding should not influence the final result. Therefore, it does not matter if we perform the padding for each batch or in the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e7dcc",
   "metadata": {
    "colab_type": "text",
    "id": "4SWxKhkVLr3P"
   },
   "source": [
    "\n",
    "## 4 - Building the Model\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Model structure\n",
    "\n",
    "\n",
    "\n",
    "Our inputs will be sentences represented as tensors that are fed to a model with:\n",
    "\n",
    "* An Embedding layer,\n",
    "* A LSTM layer\n",
    "* A Dense layer\n",
    "* A log softmax layer.\n",
    "\n",
    "We may choose between outputting only the very last LSTM output for each sentence, but we may also request the LSTM to output every value for a sentence - this is what you want. We will need every output, because the idea is to label every token in the sentence and not to predict the next token or even make an overall classification task for that sentence. \n",
    "\n",
    "This implies that when we input a single sentence, such as `[452, 3400, 123, 0, 0, 0]`, the expected output should be an array for each word ID, with a length equal to the number of tags. This output is obtained by applying the LogSoftfmax function for each of the `len(tags)` values. So, in the case of the example array with a shape of `(6,)`, the output should be an array with a shape of `(6, len(tags))`.\n",
    "\n",
    "In our case, we've seen that each sentence in the training set is 104 values long, so in a batch of, say, 64 tensors, the model shoud input a tensor of shape `(64,104)` and output another tensor with shape `(64,104,17)`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf7bc5",
   "metadata": {},
   "source": [
    "**Instructions**: You will build a function that inputs the number of tags, the vocabulary size and an optional parameter to control the embedding dimension and outputs a tensorflow model as discussed in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2493e30",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def NER(len_tags, vocab_size, embedding_dim = 50):\n",
    "    \"\"\"\n",
    "    Create a Named Entity Recognition (NER) model.\n",
    "\n",
    "    Parameters:\n",
    "    len_tags (int): Number of NER tags (output classes).\n",
    "    vocab_size (int): Vocabulary size.\n",
    "    embedding_dim (int, optional): Dimension of embedding and LSTM layers (default is 50).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): NER model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential(name = 'sequential') \n",
    "    # Add the tf.keras.layers.Embedding layer. Do not forget to mask out the zeros!\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size+1 ,embedding_dim, mask_zero = True))\n",
    "    # Add the LSTM layer. Make sure we are passing the right dimension (defined in the docstring above) \n",
    "    # and returning every output for the tf.keras.layers.LSTM layer and not the very last one.\n",
    "    model.add(tf.keras.layers.LSTM(embedding_dim,return_sequences = True))\n",
    "    # Add the final tf.keras.layers.Dense with the appropriate activation function. Remember we must pass the activation function itself ant not its call!\n",
    "    model.add(tf.keras.layers.Dense(len_tags,activation=tf.nn.log_softmax))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ab3d3",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Masked loss and metrics\n",
    "\n",
    "Before training the model, we need to create your own function to compute the accuracy. Tensorflow has built-in accuracy metrics but we cannot pass values to be ignored. This will impact the calculations, since we must remove the padded values.\n",
    "\n",
    "Usually, the metric that inputs true labels and predicted labels and outputs how many times the predicted and true labels match is called `accuracy`. In some cases, however, there is one more step before getting the predicted labels. This may happen if, instead of passing the predicted labels, a vector of probabilities is passed. In such case, there is a need to perform an `argmax` for each prediction to find the appropriate predicted label. Such situations happen very often, therefore Tensorflow has a set of functions, with prefix `Sparse`, that performs this operation in the backend. Unfortunately, it does not provide values to ignore in the accuracy case. This is what you will work on now. \n",
    "\n",
    "Note that the model's prediction has 3 axes: \n",
    "- the number of examples (batch size)\n",
    "- the number of words in each example (padded to be as long as the longest sentence in the batch)\n",
    "- the number of possible targets (the 17 named entity tags)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02eca7a",
   "metadata": {},
   "source": [
    "Another important function is the loss function. In this case, we will use the Cross Entropy loss, but we need a multiclass implementation of it, also we may look for its `Sparse` version. Tensorflow has a SparseCategoricalCrossentropy loss function, which it is already imported by the name SparseCategoricalCrossEntropy.\n",
    "\n",
    "[SparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy): The Sparse Categorical Crossentropy Loss Function. \n",
    "\n",
    "The arguments you will need:\n",
    "\n",
    "1. `from_logits`: This indicates if the values are raw values or normalized values (probabilities). Since the last layer of the model finishes with a LogSoftMax call, the results are **not** normalized - they do not lie between 0 and 1. \n",
    "2. `ignore_class`: This indicates which class should be ignored when computing the crossentropy. Remember that the class related to padding value is set to be 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1acb3d2",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class CustomSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, from_logits=False, ignore_class=-1):\n",
    "        super().__init__()\n",
    "        self.from_logits = from_logits\n",
    "        self.ignore_class = ignore_class\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure inputs are tensors\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        \n",
    "        # Generate a mask that is False where y_true equals ignore_class and True elsewhere\n",
    "        mask = tf.not_equal(y_true, self.ignore_class)\n",
    "        \n",
    "        # Use this mask to filter out ignored values from y_true and y_pred\n",
    "        y_true_filtered = tf.boolean_mask(y_true, mask)\n",
    "        y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
    "        \n",
    "        # Compute the sparse categorical crossentropy on filtered targets and predictions\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_filtered, y_pred_filtered, from_logits=self.from_logits)\n",
    "        \n",
    "        # Return the mean loss value\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the masked sparse categorical cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): True labels.\n",
    "    y_pred (tensor): Predicted logits.\n",
    "    \n",
    "    Returns:\n",
    "    loss (tensor): Calculated loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Calculate the loss for each item in the batch. Remember to pass the right arguments, as discussed above!\n",
    "    loss_fn = CustomSparseCategoricalCrossentropy(from_logits=True, ignore_class=-1)\n",
    "    # Use the previous defined function to compute the loss\n",
    "    loss = loss_fn(y_true,y_pred)\n",
    "    \n",
    "\n",
    "    return  loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb18b1a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0508584, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Let's test our Customized loss function with an example \n",
    "true_labels = [0,1,2,0]\n",
    "predicted_logits = [[0.1,0.6,0.3] , [0.2,0.7,0.1], [0.1, 0.5,0.4], [0.4,0.4,0.2]]\n",
    "print(masked_loss(true_labels, predicted_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f50fc477",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate masked accuracy for predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): True labels.\n",
    "    y_pred (tensor): Predicted logits.\n",
    "\n",
    "    Returns:\n",
    "    accuracy (tensor): Masked accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Calculate the loss for each item in the batch.\n",
    "    # We must always cast the tensors to the same type in order to use them in training. Since we will make divisions, it is safe to use tf.float32 data type.\n",
    "    y_true = tf.cast(y_true, tf.float32) \n",
    "    # Create the mask, i.e., the values that will be ignored\n",
    "    mask = tf.not_equal(y_true, -1.0)\n",
    "\n",
    "    mask = tf.cast(mask, tf.float32) \n",
    "    # Perform argmax to get the predicted values\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=-1)\n",
    "    y_pred_class = tf.cast(y_pred_class, tf.float32) \n",
    "    # Compare the true values with the predicted ones\n",
    "    matches_true_pred  = tf.equal(y_true, y_pred_class)\n",
    "    matches_true_pred = tf.cast(matches_true_pred , tf.float32) \n",
    "    # Multiply the acc tensor with the masks\n",
    "    matches_true_pred *= mask\n",
    "    # Compute masked accuracy (quotient between the total matches and the total valid values, i.e., the amount of non-masked values)\n",
    "    masked_acc = tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    return masked_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f27aa010",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# let's test the masked accuarcy\n",
    "true_labels = [0,1,2,0]\n",
    "predicted_logits = [[0.1,0.6,0.3] , [0.2,0.7,0.1], [0.1, 0.5,0.4], [0.4,0.4,0.2]]\n",
    "print(masked_accuracy(true_labels, predicted_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab287ac",
   "metadata": {},
   "source": [
    "Now we will create the model and get a summary of its parameters and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "979d44f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1492400   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 50)          20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 17)          867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513,467\n",
      "Trainable params: 1,513,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NER(len(tag_map), len(vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89d52a",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 A note on padding\n",
    "\n",
    "We will check now how padding does not affect the model's output. Of course the output dimension will change. If ten zeros are added at the end of the tensor, then the resulting output dimension will have 10 more elements (more specifically, 10 more arrays of length 17 each). However, those are removed from any calculation further on, so it won't impact at all the model's performance and training. We will be using the function tf.expand_dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c943c7b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.expand_dims(np.array([545, 467, 896]), axis = 0) # Expanding dims is needed to pass it to the model, \n",
    "                                                        # since it expects batches and not single prediction arrays\n",
    "    \n",
    "x_padded = tf.expand_dims(np.array([545, 467, 896, 0, 0, 0]), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37200a",
   "metadata": {},
   "source": [
    "Can you guess the final output prediction shape for each array defined above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8175d1c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1, 3, 17)\n",
      "x_padded shape: (1, 6, 17)\n"
     ]
    }
   ],
   "source": [
    "pred_x = model(x)\n",
    "pred_x_padded = model(x_padded)\n",
    "print(f'x shape: {pred_x.shape}\\nx_padded shape: {pred_x_padded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca0596",
   "metadata": {},
   "source": [
    "If the last three elements of `pred_x_padded` are removed, both `pred_x` and `pred_x_padded[:3]` must have the same elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bd0ad65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(pred_x, pred_x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50f793",
   "metadata": {},
   "source": [
    "Great! Now one last check: let's see that both `pred_x` and `pred_x_padded` return the same loss and accuracy values. For that, you will need a `y_true` and `y_true_padded` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c97826b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_loss is the same: True\n",
      "masked_accuracy is the same: True\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.expand_dims([16, 6, 12], axis = 0)\n",
    "y_true_padded = tf.expand_dims([16,6,12,-1,-1,-1], axis = 0) # Remember we mapped the padded values to -1 in the labels\n",
    "print(f\"masked_loss is the same: {np.allclose(masked_loss(y_true,pred_x), masked_loss(y_true_padded,pred_x_padded))}\")\n",
    "print(f\"masked_accuracy is the same: {np.allclose(masked_accuracy(y_true,pred_x), masked_accuracy(y_true_padded,pred_x_padded))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69bfc0",
   "metadata": {},
   "source": [
    "After this quick sanity check, we will now compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba387fb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), \n",
    "              loss = masked_loss,\n",
    "               metrics = [masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71405c63",
   "metadata": {
    "colab_type": "text",
    "id": "-SdkBrFVnCuV"
   },
   "source": [
    "\n",
    "### 4.4 - Training the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84b1b8be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "VU-j8hs-nCue",
    "outputId": "fbbbda7d-b6dd-42e4-a4c6-58c0a6e349b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525/525 [==============================] - 67s 103ms/step - loss: 0.0561 - masked_accuracy: 0.9309 - val_loss: 0.0418 - val_masked_accuracy: 0.9583\n",
      "Epoch 2/2\n",
      "525/525 [==============================] - 50s 94ms/step - loss: 0.0229 - masked_accuracy: 0.9661 - val_loss: 0.0402 - val_masked_accuracy: 0.9592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231d8e2c160>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(33) ## Setting again a random seed to ensure reproducibility\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(train_dataset.batch(BATCH_SIZE),\n",
    "          validation_data = val_dataset.batch(BATCH_SIZE),\n",
    "          shuffle=True,\n",
    "          epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270ad1f",
   "metadata": {
    "colab_type": "text",
    "id": "c4r-gXOZLr3j"
   },
   "source": [
    "\n",
    "## 5 - Compute Accuracy\n",
    "\n",
    "Let's  evaluate on the test set. Previously, we have seen the accuracy on the training set and the validation (noted as eval) set. we will now evaluate on our test set. We already have a function to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78687bf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "yCWFwt3m1sgL",
    "outputId": "701d6b4d-b9b6-41f7-80b3-d0c043880704",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "# Convert the sentences into ids\n",
    "test_sentences_id = sentence_vectorizer(test_sentences)\n",
    "# Convert the labels into token ids\n",
    "test_labels_id = label_vectorizer(test_labels,tag_map)\n",
    "# Rename to prettify next function call\n",
    "y_true = test_labels_id \n",
    "y_pred = model.predict(test_sentences_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad366122",
   "metadata": {},
   "source": [
    "The next cell computes the accuracy for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eff0e106",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy in test set is: 95.82%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model's accuracy in test set is: {100*masked_accuracy(y_true,y_pred).numpy():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578a163",
   "metadata": {
    "colab_type": "text",
    "id": "EOeTPAx_Lr3t"
   },
   "source": [
    "\n",
    "## 6 - Testing with your Own Sentence\n",
    "\n",
    "In this section you will make a predictor function to predict the NER labels for any sentence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14b7025a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "0K4SyB20cHRf",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(sentence, model, sentence_vectorizer, tag_map):\n",
    "    \"\"\"\n",
    "    Predict NER labels for a given sentence using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): Input sentence.\n",
    "    model (tf.keras.Model): Trained NER model.\n",
    "    sentence_vectorizer (tf.keras.layers.TextVectorization): Sentence vectorization layer.\n",
    "    tag_map (dict): Dictionary mapping tag IDs to labels.\n",
    "\n",
    "    Returns:\n",
    "    predictions (list): Predicted NER labels for the sentence.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the sentence into ids\n",
    "    sentence_vectorized = sentence_vectorizer(sentence)\n",
    "    # Expand its dimension to make it appropriate to pass to the model\n",
    "    sentence_vectorized = tf.expand_dims(sentence_vectorized, axis=0)\n",
    "    # Get the model output\n",
    "    output = model.predict(sentence_vectorized)\n",
    "    # Get the predicted labels for each token, using argmax function and specifying the correct axis to perform the argmax\n",
    "    outputs = np.argmax(output, axis = -1)\n",
    "    # Next line is just to adjust outputs dimension. Since this function expects only one input to get a prediction, outputs will be something like [[1,2,3]]\n",
    "    # so to avoid heavy notation below, let's transform it into [1,2,3]\n",
    "    outputs = outputs[0] \n",
    "    # Get a list of all keys, remember that the tag_map was built in a way that each label id matches its index in a list\n",
    "    labels = list(tag_map.keys()) \n",
    "    pred = [] \n",
    "    # Iterating over every predicted token in outputs list\n",
    "    for tag_idx in outputs:\n",
    "        pred_label = labels[tag_idx]\n",
    "        pred.append(pred_label)\n",
    "    \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1be0189a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "deletable": false,
    "id": "vLZCHoiULr3u",
    "outputId": "fab815fd-0472-4eaf-968a-abff1f5cfff5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "French B-gpe\n",
      "Morocco B-geo\n",
      "summer B-tim\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "##                   Try Your Own Sentence             ##\n",
    "#########################################################\n",
    "\n",
    "\n",
    "sentence=\"Many French citizens are goin to visit Morocco for summer\"\n",
    "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
